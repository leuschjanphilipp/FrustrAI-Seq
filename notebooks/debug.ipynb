{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1651548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "sys.path.append('..')\n",
    "from pLMtrainer.dataloader import FrustrationDataset, FrustrationDataModule\n",
    "from pLMtrainer.models import FrustrationFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f639ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"../data/frustration/v3_frustration.parquet.gzip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df14fb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proteinID</th>\n",
       "      <th>full_seq</th>\n",
       "      <th>res_seq</th>\n",
       "      <th>res_idx</th>\n",
       "      <th>frst_idx</th>\n",
       "      <th>frst_class</th>\n",
       "      <th>set</th>\n",
       "      <th>cath_T_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF-A0A009EQP3-F1-model_v4_TED02</td>\n",
       "      <td>MKESLRLRLDQLSDRHEELTALLADVEVISDNKRFRQLSREHNDLT...</td>\n",
       "      <td>[Y, L, E, I, R, A, G, T, G, G, D, E, A, A, I, ...</td>\n",
       "      <td>[111, 112, 113, 114, 115, 116, 117, 118, 119, ...</td>\n",
       "      <td>[0.235, 1.632, -0.844, 1.365, 0.282, -0.384, 0...</td>\n",
       "      <td>[9, 6, 11, 6, 9, 10, 9, 9, 9, 11, 12, 13, 9, 8...</td>\n",
       "      <td>train</td>\n",
       "      <td>3.30.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF-A0A009F754-F1-model_v4_TED02</td>\n",
       "      <td>MANPAQLVRHKLLNTFFSRHSVWFACITIAVIFTIFHIGYEPRYIY...</td>\n",
       "      <td>[R, I, L, I, G, N, E, Q, C, T, Q, P, Y, S, A, ...</td>\n",
       "      <td>[164, 165, 166, 167, 168, 169, 170, 171, 172, ...</td>\n",
       "      <td>[-1.068, 1.469, -0.317, 1.098, -0.663, -0.771,...</td>\n",
       "      <td>[12, 6, 10, 7, 11, 11, 7, 10, 3, 9, 13, 11, 9,...</td>\n",
       "      <td>train</td>\n",
       "      <td>3.20.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         proteinID  \\\n",
       "0  AF-A0A009EQP3-F1-model_v4_TED02   \n",
       "1  AF-A0A009F754-F1-model_v4_TED02   \n",
       "\n",
       "                                            full_seq  \\\n",
       "0  MKESLRLRLDQLSDRHEELTALLADVEVISDNKRFRQLSREHNDLT...   \n",
       "1  MANPAQLVRHKLLNTFFSRHSVWFACITIAVIFTIFHIGYEPRYIY...   \n",
       "\n",
       "                                             res_seq  \\\n",
       "0  [Y, L, E, I, R, A, G, T, G, G, D, E, A, A, I, ...   \n",
       "1  [R, I, L, I, G, N, E, Q, C, T, Q, P, Y, S, A, ...   \n",
       "\n",
       "                                             res_idx  \\\n",
       "0  [111, 112, 113, 114, 115, 116, 117, 118, 119, ...   \n",
       "1  [164, 165, 166, 167, 168, 169, 170, 171, 172, ...   \n",
       "\n",
       "                                            frst_idx  \\\n",
       "0  [0.235, 1.632, -0.844, 1.365, 0.282, -0.384, 0...   \n",
       "1  [-1.068, 1.469, -0.317, 1.098, -0.663, -0.771,...   \n",
       "\n",
       "                                          frst_class    set cath_T_id  \n",
       "0  [9, 6, 11, 6, 9, 10, 9, 9, 9, 11, 12, 13, 9, 8...  train   3.30.70  \n",
       "1  [12, 6, 10, 7, 11, 11, 7, 10, 3, 9, 13, 11, 9,...  train   3.20.20  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pq.read_table(parquet_path).to_pandas()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8246383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = FrustrationDataModule(parquet_path=parquet_path, regression=False, batch_size=10, max_seq_length=100, num_workers=1, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28ac373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using half precision for the pLM encoder\n"
     ]
    }
   ],
   "source": [
    "model = FrustrationFNN(input_dim=1024, \n",
    "                       hidden_dim=32, \n",
    "                       output_dim=20, \n",
    "                       dropout=0.15, \n",
    "                       max_seq_length=100,\n",
    "                       regression=False, \n",
    "                       pLM_model=\"../data/ProstT5\", \n",
    "                       pLM_precision=\"half\", \n",
    "                       prefix_prostT5=\"<AA2fold>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b536b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=5,\n",
    "                           mode='min',\n",
    "                           verbose=True)\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                             dirpath=\"./checkpoints\",\n",
    "                             filename=f\"debug\",\n",
    "                             save_top_k=1,\n",
    "                             mode='min',\n",
    "                             save_weights_only=True)\n",
    "logger = CSVLogger(\"./checkpoints\", name=\"debug_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff6c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from ../data/frustration/v3_frustration.parquet.gzip\n",
      "Created train/val/test masks\n",
      "Initialized res_idx_mask and frst_vals tensors\n",
      "Populated res_idx_mask and frst_vals tensors\n",
      "Created train dataset\n",
      "Created val dataset\n",
      "Created test dataset\n",
      "Train/Val/Test split: 899/35/66 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /Users/janleusch/Documents/phd/pLMtrainer/pLMtrainer/notebooks/checkpoints exists and is not empty.\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | T5EncoderModel   | 1.2 B  | eval \n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "2 | FNN     | Sequential       | 33.5 K | train\n",
      "-----------------------------------------------------\n",
      "1.2 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,832.791 Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "439       Modules in eval mode\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | T5EncoderModel   | 1.2 B  | eval \n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "2 | FNN     | Sequential       | 33.5 K | train\n",
      "-----------------------------------------------------\n",
      "1.2 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,832.791 Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "439       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring optimizers\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.9851627349853516\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.83it/s]Validation loss: 2.9802629947662354\n",
      "Validation loss: 2.9802629947662354                                        \n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 439 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 439 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 90/90 [01:28<00:00,  1.02it/s, v_num=6, train_loss_step=1.890]Validation loss: 1.8892680406570435\n",
      "Validation loss: 1.8892680406570435\n",
      "Validation loss: 1.9481663703918457\n",
      "Validation loss: 1.9481663703918457\n",
      "Validation loss: 1.902886152267456\n",
      "Validation batch with no valid residues - skipping\n",
      "Validation loss: 1.902886152267456\n",
      "Validation batch with no valid residues - skipping\n",
      "Epoch 0: 100%|██████████| 90/90 [01:31<00:00,  0.98it/s, v_num=6, train_loss_step=1.890, val_loss=1.910, train_loss_epoch=2.310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 90/90 [01:35<00:00,  0.94it/s, v_num=6, train_loss_step=1.810, val_loss=1.910, train_loss_epoch=2.310]Validation loss: 1.753843069076538\n",
      "Validation loss: 1.753843069076538\n",
      "Validation loss: 1.8118500709533691\n",
      "Validation loss: 1.8118500709533691\n",
      "Validation loss: 1.7686442136764526\n",
      "Validation batch with no valid residues - skipping\n",
      "Epoch 1: 100%|██████████| 90/90 [01:38<00:00,  0.91it/s, v_num=6, train_loss_step=1.810, val_loss=1.780, train_loss_epoch=1.850]Validation loss: 1.7686442136764526\n",
      "Validation batch with no valid residues - skipping\n",
      "Epoch 1: 100%|██████████| 90/90 [01:38<00:00,  0.91it/s, v_num=6, train_loss_step=1.810, val_loss=1.780, train_loss_epoch=1.850]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.135 >= min_delta = 0.0. New best score: 1.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 90/90 [01:46<00:00,  0.85it/s, v_num=6, train_loss_step=1.740, val_loss=1.780, train_loss_epoch=1.850]Validation loss: 1.7067312002182007\n",
      "Validation loss: 1.7067312002182007\n",
      "Validation loss: 1.7602545022964478\n",
      "Validation loss: 1.7602545022964478\n",
      "Validation loss: 1.7078973054885864\n",
      "Validation batch with no valid residues - skipping\n",
      "Validation loss: 1.7078973054885864\n",
      "Validation batch with no valid residues - skipping\n",
      "Epoch 2: 100%|██████████| 90/90 [01:50<00:00,  0.82it/s, v_num=6, train_loss_step=1.740, val_loss=1.720, train_loss_epoch=1.760]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.053 >= min_delta = 0.0. New best score: 1.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 90/90 [01:48<00:00,  0.83it/s, v_num=6, train_loss_step=1.730, val_loss=1.720, train_loss_epoch=1.760]Validation loss: 1.6805704832077026\n",
      "Validation loss: 1.6805704832077026\n",
      "Validation loss: 1.7211381196975708\n",
      "Validation loss: 1.7211381196975708\n",
      "Validation loss: 1.675698161125183\n",
      "Validation batch with no valid residues - skipping\n",
      "\n",
      "Validation loss: 1.675698161125183\n",
      "Epoch 3: 100%|██████████| 90/90 [01:52<00:00,  0.80it/s, v_num=6, train_loss_step=1.730, val_loss=1.690, train_loss_epoch=1.710]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 1.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 90/90 [01:50<00:00,  0.81it/s, v_num=6, train_loss_step=1.790, val_loss=1.690, train_loss_epoch=1.710]Validation loss: 1.6630829572677612\n",
      "Validation loss: 1.6630829572677612\n",
      "Validation loss: 1.6932507753372192\n",
      "Validation loss: 1.6932507753372192\n",
      "Validation loss: 1.659364938735962\n",
      "Validation batch with no valid residues - skipping\n",
      "Validation loss: 1.659364938735962\n",
      "Validation batch with no valid residues - skipping\n",
      "Epoch 4: 100%|██████████| 90/90 [01:54<00:00,  0.78it/s, v_num=6, train_loss_step=1.790, val_loss=1.670, train_loss_epoch=1.670]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 1.672\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 90/90 [01:56<00:00,  0.77it/s, v_num=6, train_loss_step=1.790, val_loss=1.670, train_loss_epoch=1.670]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator='auto', # gpu\n",
    "                  devices=-1, # 4 for one node on haicore\n",
    "                  #strategy='ddp',\n",
    "                  max_epochs=5,\n",
    "                  logger=logger,\n",
    "                  log_every_n_steps=10, # 50 for haicore default\n",
    "                  callbacks=[early_stop, checkpoint],\n",
    "                  precision=\"16-mixed\",\n",
    "                  gradient_clip_val=1,\n",
    "                  enable_progress_bar=True,\n",
    "                  deterministic=False, # for reproducibility disable on cluster \n",
    "                  #num_sanity_val_steps=0,\n",
    "                  #accumulate_grad_batches=2, # if batch size gets too small --> test on H100/A100\n",
    "                  )\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c99b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 982852 samples from ../data/frustration/v3_frustration.parquet.gzip\n",
      "Train/Val/Test split: 896034/29926/56892 samples\n"
     ]
    }
   ],
   "source": [
    "data_module.setup()\n",
    "for batch in data_module.val_dataloader():\n",
    "    full_seq, res_mask, frst_vals = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8a06c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MIDQIKRHGLFDIDIHCDGDLEIDDHHTVEDCGITLGQAFAQALGDKKGLRRYGHFYAPLDEALSRVVVDLSGRPGLFMDIPFTRARIGTFDVDLFSEFFQGFVNHALMTLHIDNLKGKNSHHQIESVFKALARALRMACEIDPRAENTIASTKGSL',)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "768543a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_seq = (\"SEQVE\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37eaf6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<AA2fold> S E Q V E']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [\"<AA2fold>\" + \" \" + \" \".join(seq) for seq in eg_seq]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f84aa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"../data/prostT5\", do_lower_case=False, max_length=10)\n",
    "ids = tokenizer.batch_encode_plus(seq, \n",
    "                                add_special_tokens=True, \n",
    "                                padding=\"max_length\",\n",
    "                                truncation=\"longest_first\", \n",
    "                                max_length=10,\n",
    "                                return_tensors='pt'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22f02b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[149,   7,   9,  16,   6,   9,   1,   0,   0,   0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[\"input_ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biotrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
