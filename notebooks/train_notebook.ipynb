{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd050106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('FrustraSeq')\n",
    "from FrustraSeq.models.FrustraSeq import FrustraSeq\n",
    "from FrustraSeq.dataloader import FrustrationDataModule\n",
    "from FrustraSeq.utils import run_eval_metrics\n",
    "\n",
    "config = {\n",
    "    \"experiment_name\": \"it5_DEB_FOCAL\",\n",
    "    \"parquet_path\": \"../data/frustration/v8_frustration_v2.parquet.gzip\",\n",
    "    \"set_key\": \"split_test\", # split_test (gonzalos prots in test) or set_old (split for previous dataset) or split0-3\n",
    "    \"cath_sampling_n\": 10, # 100,  # None for no sampling\n",
    "    \"batch_size\": 10, #32 for FT; 512 else\n",
    "    \"num_workers\": 10,\n",
    "    \"max_seq_length\": 100,\n",
    "    \"precision\": \"full\",\n",
    "    \"pLM_model\": \"../data/protT5\",\n",
    "    \"prefix_prostT5\": \"<AA2fold>\",\n",
    "    \"pLM_dim\": 1024, #1280 for ESM\n",
    "    \"no_label_token\": -100,\n",
    "    \"finetune\": False,\n",
    "    \"lora_r\": 4,\n",
    "    \"lora_alpha\": 1,\n",
    "    \"lora_modules\": [\"q\", \"k\", \"v\", \"o\"], #\"wi\", \"wo\", \"w1\", \"w2\", \"w3\", \"fc1\", \"fc2\", \"fc3\"], # [\"query\", \"key\", \"value\", \"fc1\", \"fc2\"] for esm\n",
    "    \"ce_weighting\": None, #[2.65750085, 0.68876299, 0.8533673], #[10.0, 2.0, 2.5], [(1/0.13)/(1/0.13), (1/0.48)/(1/0.13), (1/0.39)/(1/0.13)]\n",
    "    \"use_focal_loss_instead_of_ce\": True,\n",
    "    \"notes\": \"\",\n",
    "}\n",
    "architecture = {}\n",
    "architecture[\"lr\"] = 1e-4\n",
    "architecture[\"dropout\"] = 0.1\n",
    "architecture[\"kernel_1\"] = 7\n",
    "architecture[\"padding_1\"] = architecture[\"kernel_1\"] // 2  # to keep same length\n",
    "architecture[\"kernel_2\"] = 7\n",
    "architecture[\"padding_2\"] = architecture[\"kernel_2\"] // 2  # to keep same length\n",
    "architecture[\"hidden_dim_0\"] = 64\n",
    "architecture[\"hidden_dim_1\"] = 10\n",
    "config[\"architecture\"] = architecture\n",
    "\n",
    "#torch.set_float32_matmul_precision(\"high\")\n",
    "trainer_precision = \"bf16-mixed\" #\"32\"\n",
    "\n",
    "if config[\"finetune\"]:\n",
    "    find_unused = False\n",
    "else:\n",
    "    find_unused = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d085f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = FrustrationDataModule(df=None,\n",
    "                                    parquet_path=config[\"parquet_path\"], \n",
    "                                    batch_size=config[\"batch_size\"],\n",
    "                                    set_key=config[\"set_key\"],\n",
    "                                    max_seq_length=config[\"max_seq_length\"], \n",
    "                                    num_workers=config[\"num_workers\"], # 0 \n",
    "                                    persistent_workers=True, # Flase\n",
    "                                    pin_memory=True, # Flase\n",
    "                                    prefetch_factor=1, #!\n",
    "                                    sample_size=None,\n",
    "                                    cath_sampling_n=config[\"cath_sampling_n\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
    "                            patience=5,\n",
    "                            min_delta=0.0001,\n",
    "                            mode='min',\n",
    "                            verbose=True)\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                dirpath=f\"./{config['experiment_name']}\",\n",
    "                                filename=f\"best_val_model\",\n",
    "                                save_top_k=1,\n",
    "                                mode='min',\n",
    "                                save_weights_only=False)\n",
    "logger = WandbLogger(project=\"FrustraSeq\",\n",
    "                        name=config[\"experiment_name\"],\n",
    "                        save_dir=f\"./{config['experiment_name']}\",\n",
    "                        log_model=False,\n",
    "                        offline=True,\n",
    "                        )\n",
    "lr_logger = LearningRateMonitor(logging_interval='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK -1: Starting new training run\n",
      "Using focal loss instead of cross-entropy loss for classification head. Overrides ce_weighting if set.\n",
      "RANK -1: Model initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id uj5him43.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>./it5_DEB_FOCAL/wandb/offline-run-20251222_175657-uj5him43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5620 samples from ../data/frustration/v8_frustration_v2.parquet.gzip\n",
      "Created train/val/test masks\n",
      "Initialized res_idx_mask and frst_vals tensors\n",
      "Populated res_idx_mask and frst_vals tensors\n",
      "Created train dataset\n",
      "Created val dataset\n",
      "Created test dataset\n",
      "Train/Val/Test split: 4990/300/330 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /Users/janleusch/Documents/phd/pLMtrainer/FrustraSeq/notebooks/it5_DEB_FOCAL exists and is not empty.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK -1: lora params: 0, head params: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name        | Type           | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | encoder     | T5EncoderModel | 1.2 B  | eval \n",
      "1 | CNN         | Sequential     | 3.2 M  | train\n",
      "2 | reg_head    | Sequential     | 11     | train\n",
      "3 | cls_head    | Sequential     | 33     | train\n",
      "4 | mse_loss_fn | MSELoss        | 0      | train\n",
      "5 | ce_loss_fn  | FocalLoss      | 0      | train\n",
      "-------------------------------------------------------\n",
      "1.2 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 B     Total params\n",
      "4,845.538 Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "439       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation 2025-12-22 17:57:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val batch with no valid residues - skipping\n",
      "Ending validation 2025-12-22 17:57:59\n",
      "Starting training epoch 0 at 2025-12-22 17:57:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 439 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation 2025-12-22 18:00:20\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending validation 2025-12-22 18:01:09\n",
      "Starting validation 2025-12-22 18:04:03\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.320 >= min_delta = 0.0001. New best score: 0.570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending validation 2025-12-22 18:04:49\n",
      "Starting validation 2025-12-22 18:07:25\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.060 >= min_delta = 0.0001. New best score: 0.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending validation 2025-12-22 18:08:10\n",
      "Starting validation 2025-12-22 18:10:44\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n",
      "Val batch with no valid residues - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.0001. New best score: 0.492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending validation 2025-12-22 18:11:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janleusch/anaconda3/envs/biotrainer/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x320ca9550>> (for post_run_cell), with arguments args (<ExecutionResult object at 311f1e120, execution_count=3 error_before_exec=None error_in_exec=1 info=<ExecutionInfo object at 311f1efc0, raw_cell=\"trainer = Trainer(default_root_dir=f\"./{config['ex..\" transformed_cell=\"trainer = Trainer(default_root_dir=f\"./{config['ex..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/janleusch/Documents/phd/pLMtrainer/FrustraSeq/notebooks/train_notebook.ipynb#W4sZmlsZQ%3D%3D> result=None>,),kwargs {}:"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(default_root_dir=f\"./{config['experiment_name']}\",\n",
    "                accelerator=\"mps\",\n",
    "                devices=1,\n",
    "                #strategy=DDPStrategy(find_unused_parameters=find_unused),\n",
    "                max_epochs=1,\n",
    "                logger=logger,\n",
    "                log_every_n_steps=10,\n",
    "                val_check_interval=0.2,\n",
    "                callbacks=[early_stop, checkpoint, lr_logger],\n",
    "                precision=trainer_precision,\n",
    "                gradient_clip_val=1,\n",
    "                enable_progress_bar=False,\n",
    "                deterministic=False,\n",
    "                accumulate_grad_batches=1, # used 8 for FT, maybe less for for FT in future.\n",
    "                )\n",
    "\n",
    "ckpt_path = None\n",
    "ckpt_file = f\"{config['experiment_name']}/best_val_model.ckpt\"\n",
    "\n",
    "if os.path.exists(ckpt_file):\n",
    "    ckpt_path = ckpt_file\n",
    "    print(f\"RANK {os.environ.get('RANK', -1)}: Resuming training from checkpoint: {ckpt_file}\")\n",
    "else:\n",
    "    print(f\"RANK {os.environ.get('RANK', -1)}: Starting new training run\")\n",
    "\n",
    "model = FrustraSeq(config=config)\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=data_module,\n",
    "    ckpt_path=ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf468f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trainer = Trainer(accelerator=\"gpu\", # gpu\n",
    "                        devices=1, # only use one gpu for inference\n",
    "                        max_epochs=2, \n",
    "                        logger=logger,\n",
    "                        log_every_n_steps=10,\n",
    "                        val_check_interval=0.2, \n",
    "                        precision=trainer_precision, #!, config[\"inference_precision\"],\n",
    "                        gradient_clip_val=1,\n",
    "                        enable_progress_bar=False,\n",
    "                        )\n",
    "#tune on val set\n",
    "data_module.test_dataloader = data_module.val_dataloader\n",
    "model = FrustraSeq.load_from_checkpoint(checkpoint_path=f\"{config['experiment_name']}/best_val_model.ckpt\",\n",
    "                                        config=config)\n",
    "test_trainer.test(model, datamodule=data_module)\n",
    "model.save_preds_dict(set=\"val\")\n",
    "\n",
    "metrics = run_eval_metrics(np.load(f\"./{config['experiment_name']}/val_preds.npz\"), return_cls_report_dict=False)\n",
    "print(metrics[\"cls_report\"])\n",
    "print(metrics[\"pearson_r\"])\n",
    "print(metrics[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff5da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cc3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41278bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biotrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
